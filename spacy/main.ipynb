{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy import displacy\n",
    "from spacy.util import filter_spans\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from rich import print as prt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in utils.v2Tov3Converter(train_data):\n",
    "#     for j in i.ents:\n",
    "#         prt(j.text, \" \", j.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('train_data/train_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    # convert tuples to list and return converted data\n",
    "    def prepareData(train_data):\n",
    "        data = []\n",
    "        for text, annot in train_data:\n",
    "            ent = []\n",
    "            for strt, end, lbl in annot['entities']:\n",
    "                ent.append([strt, end, lbl])\n",
    "            annot['entities'] = ent\n",
    "            data.append([text, annot])\n",
    "        return data\n",
    "\n",
    "    # visualise training data using displacy\n",
    "    def renderData(data, start=0, end=1, serve=False):\n",
    "        nlp = spacy.blank('en')\n",
    "        data0 = data[start:end]\n",
    "        for text, annotations in data0:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if(span!=None):\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "        if serve:\n",
    "            displacy.serve(doc, style='ent')\n",
    "        else:\n",
    "            displacy.render(doc, style='ent')\n",
    "\n",
    "    def remove_whitespace_entities(doc):\n",
    "        doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "        return doc\n",
    "\n",
    "    # convert training dataset from v2 to v3 using docbin\n",
    "    # note use filter_span to to get rid of the span errors\n",
    "    def v2Tov3Converter(data, filename=\"train\"):\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        # the DocBin will store the example documents\n",
    "        db = DocBin()\n",
    "        for text, annotations in data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                # span = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span == None:\n",
    "                    continue\n",
    "                if span.text.isspace()==True:\n",
    "                    continue\n",
    "                proceed = True\n",
    "                span_text = span.text\n",
    "                if span_text[0]==' ' or span_text[len(span_text)-1]==' ':\n",
    "                    continue\n",
    "                for char in span.text:\n",
    "                    if char.isalnum():\n",
    "                        continue\n",
    "                    else:\n",
    "                        proceed = False\n",
    "\n",
    "                if proceed:\n",
    "                    ents.append(span)\n",
    "\n",
    "            ents = filter_spans(ents)\n",
    "            # prt(ents)\n",
    "            doc.ents = ents\n",
    "            doc = utils.remove_whitespace_entities(doc)\n",
    "            db.add(doc)\n",
    "        filename=filename+\".spacy\"\n",
    "        db.to_disk(filename)\n",
    "        return list(db.get_docs(nlp.vocab))\n",
    "    \n",
    "    def trim_entity_spans(data: list) -> list:\n",
    "        # Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "        # Args:\n",
    "        # data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "        # Returns:\n",
    "        # list: The cleaned data.\n",
    "        invalid_span_tokens = re.compile(r'\\s')\n",
    "        cleaned_data = []\n",
    "        for text, annotations in data:\n",
    "            entities = annotations['entities']\n",
    "            valid_entities = []\n",
    "            for start, end, label in entities:\n",
    "                valid_start = start\n",
    "                valid_end = end\n",
    "                # if there's preceding spaces, move the start position to nearest character\n",
    "                while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                        text[valid_start]):\n",
    "                    valid_start += 1\n",
    "                while valid_end > 1 and invalid_span_tokens.match(\n",
    "                        text[valid_end - 1]):\n",
    "                    valid_end -= 1\n",
    "\n",
    "                valid_entities.append([valid_start, valid_end, label])\n",
    "            cleaned_data.append([text, {'entities': valid_entities}])\n",
    "        return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = utils.v2Tov3Converter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# initialize config.cfg file\n",
    "!spacy init config --lang en --pipeline ner config.cfg --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using config.cfg file. and save the model in trained_model folder.\n",
    "!python -m spacy train config.cfg --output ./trained_model/ --paths.train ./train.spacy --paths.dev ./train.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
