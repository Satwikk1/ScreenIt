{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy import displacy\n",
    "from spacy.util import filter_spans\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from rich import print as prt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in utils.v2Tov3Converter(train_data):\n",
    "#     for j in i.ents:\n",
    "#         prt(j.text, \" \", j.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('train_data/train_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    # convert tuples to list and return converted data\n",
    "    def prepareData(train_data):\n",
    "        data = []\n",
    "        for text, annot in train_data:\n",
    "            ent = []\n",
    "            for strt, end, lbl in annot['entities']:\n",
    "                ent.append([strt, end, lbl])\n",
    "            annot['entities'] = ent\n",
    "            data.append([text, annot])\n",
    "        return data\n",
    "\n",
    "    # visualise training data using displacy\n",
    "    def renderData(data, start=0, end=1, serve=False):\n",
    "        nlp = spacy.blank('en')\n",
    "        data0 = data[start:end]\n",
    "        for text, annotations in data0:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if(span!=None):\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "        if serve:\n",
    "            displacy.serve(doc, style='ent')\n",
    "        else:\n",
    "            displacy.render(doc, style='ent')\n",
    "\n",
    "    def remove_whitespace_entities(doc):\n",
    "        doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "        return doc\n",
    "\n",
    "    # convert training dataset from v2 to v3 using docbin\n",
    "    # note use filter_span to to get rid of the span errors\n",
    "    def v2Tov3Converter(data, filename=\"train\"):\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        # the DocBin will store the example documents\n",
    "        db = DocBin()\n",
    "        for text, annotations in data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                # span = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span == None:\n",
    "                    continue\n",
    "                if span.text.isspace()==True:\n",
    "                    continue\n",
    "                proceed = True\n",
    "                span_text = span.text\n",
    "                if span_text[0]==' ' or span_text[len(span_text)-1]==' ':\n",
    "                    continue\n",
    "                for char in span.text:\n",
    "                    if char.isalnum():\n",
    "                        continue\n",
    "                    else:\n",
    "                        proceed = False\n",
    "\n",
    "                if proceed:\n",
    "                    ents.append(span)\n",
    "\n",
    "            ents = filter_spans(ents)\n",
    "            # prt(ents)\n",
    "            doc.ents = ents\n",
    "            doc = utils.remove_whitespace_entities(doc)\n",
    "            db.add(doc)\n",
    "        filename=filename+\".spacy\"\n",
    "        db.to_disk(filename)\n",
    "        return list(db.get_docs(nlp.vocab))\n",
    "    \n",
    "    def trim_entity_spans(data: list) -> list:\n",
    "        # Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "        # Args:\n",
    "        # data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "        # Returns:\n",
    "        # list: The cleaned data.\n",
    "        invalid_span_tokens = re.compile(r'\\s')\n",
    "        cleaned_data = []\n",
    "        for text, annotations in data:\n",
    "            entities = annotations['entities']\n",
    "            valid_entities = []\n",
    "            for start, end, label in entities:\n",
    "                valid_start = start\n",
    "                valid_end = end\n",
    "                # if there's preceding spaces, move the start position to nearest character\n",
    "                while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                        text[valid_start]):\n",
    "                    valid_start += 1\n",
    "                while valid_end > 1 and invalid_span_tokens.match(\n",
    "                        text[valid_end - 1]):\n",
    "                    valid_end -= 1\n",
    "\n",
    "                valid_entities.append([valid_start, valid_end, label])\n",
    "            cleaned_data.append([text, {'entities': valid_entities}])\n",
    "        return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = utils.v2Tov3Converter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# initialize config.cfg file\n",
    "!spacy init config --lang en --pipeline ner config.cfg --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ No output directory provided\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-11-02 12:07:56,221] [INFO] Set up nlp object from config\n",
      "[2021-11-02 12:07:56,248] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-11-02 12:07:56,260] [INFO] Created vocabulary\n",
      "[2021-11-02 12:07:56,277] [INFO] Finished initializing nlp object\n",
      "[2021-11-02 12:08:04,595] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/_util.py\", line 69, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/typer/main.py\", line 500, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/train.py\", line 63, in train_cli\n",
      "    train(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/training/loop.py\", line 122, in train\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/training/loop.py\", line 105, in train\n",
      "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/training/loop.py\", line 203, in train_while_improving\n",
      "    nlp.update(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py\", line 1122, in update\n",
      "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])\n",
      "  File \"spacy/pipeline/transition_parser.pyx\", line 387, in spacy.pipeline.transition_parser.Parser.update\n",
      "  File \"spacy/pipeline/transition_parser.pyx\", line 643, in spacy.pipeline.transition_parser.Parser._init_gold_batch\n",
      "  File \"spacy/pipeline/_parser_internals/transition_system.pyx\", line 132, in spacy.pipeline._parser_internals.transition_system.TransitionSystem.get_oracle_sequence_from_state\n",
      "ValueError: [E024] Could not find an optimal move to supervise the parser. Usually, this means that the model can't be updated in a way that's valid and satisfies the correct annotations specified in the GoldParse. For example, are all labels added to the model? If you're training a named entity recognizer, also make sure that none of your annotated entity spans have leading or trailing whitespace or punctuation. You can also use the `debug data` command to validate your JSON-formatted training data. For details, run:\n",
      "python -m spacy debug data --help\n"
     ]
    }
   ],
   "source": [
    "# train the model using config.cfg file. and save the model in trained_model folder.\n",
    "!python -m spacy train config.cfg --output ./trained_model/ --paths.train ./train.spacy --paths.dev ./train.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
