{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy import displacy\n",
    "from spacy.util import filter_spans\n",
    "import pickle\n",
    "import fitz\n",
    "import ast\n",
    "import pathlib as pl\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from rich import print as prt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('train_data/train_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    # convert tuples to list and return converted data\n",
    "    def prepareData(train_data):\n",
    "        data = []\n",
    "        for text, annot in train_data:\n",
    "            ent = []\n",
    "            for strt, end, lbl in annot['entities']:\n",
    "                ent.append([strt, end, lbl])\n",
    "            annot['entities'] = ent\n",
    "            data.append([text, annot])\n",
    "        return data\n",
    "\n",
    "    # visualise training data using displacy\n",
    "    def renderData(data, start=0, end=1, serve=False):\n",
    "        nlp = spacy.blank('en')\n",
    "        data0 = data[start:end]\n",
    "        for text, annotations in data0:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if(span!=None):\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "        if serve:\n",
    "            displacy.serve(doc, style='ent')\n",
    "        else:\n",
    "            displacy.render(doc, style='ent')\n",
    "\n",
    "    def remove_whitespace_entities(doc):\n",
    "        doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "        return doc\n",
    "\n",
    "    # convert training dataset from v2 to v3 using docbin\n",
    "    # note use filter_span to to get rid of the span errors\n",
    "    def v2Tov3Converter(data, filename=\"train\"):\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        # the DocBin will store the example documents\n",
    "        db = DocBin()\n",
    "        for text, annotations in data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                # span = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span == None:\n",
    "                    continue\n",
    "                if span.text.isspace()==True:\n",
    "                    continue\n",
    "                proceed = True\n",
    "                span_text = span.text\n",
    "                if span_text[0]==' ' or span_text[len(span_text)-1]==' ':\n",
    "                    continue\n",
    "                for char in span.text:\n",
    "                    if char.isalnum():\n",
    "                        continue\n",
    "                    else:\n",
    "                        proceed = False\n",
    "\n",
    "                if proceed:\n",
    "                    ents.append(span)\n",
    "\n",
    "            ents = filter_spans(ents)\n",
    "            # prt(ents)\n",
    "            doc.ents = ents\n",
    "            doc = utils.remove_whitespace_entities(doc)\n",
    "            db.add(doc)\n",
    "        filename=filename+\".spacy\"\n",
    "        db.to_disk(filename)\n",
    "        return list(db.get_docs(nlp.vocab))\n",
    "    \n",
    "    def initializeConfig():\n",
    "        # initialize config.cfg file\n",
    "        os.system(\"spacy init config --lang en --pipeline ner config.cfg --force\")\n",
    "    \n",
    "    def trainModel():\n",
    "        # train the model using config.cfg file. and save the model in trained_model folder.\n",
    "        os.system(\"spacy train config.cfg --output ./trained_model/ --paths.train ./train.spacy --paths.dev ./train.spacy\")\n",
    "    \n",
    "    def trim_entity_spans(data: list) -> list:\n",
    "        # Removes leading and trailing white spaces from entity spans.\n",
    "        \n",
    "        # Args:\n",
    "        # data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "        # Returns:\n",
    "        # list: The cleaned data.\n",
    "        invalid_span_tokens = re.compile(r'\\s')\n",
    "        cleaned_data = []\n",
    "        for text, annotations in data:\n",
    "            entities = annotations['entities']\n",
    "            valid_entities = []\n",
    "            for start, end, label in entities:\n",
    "                valid_start = start\n",
    "                valid_end = end\n",
    "                # if there's preceding spaces, move the start position to nearest character\n",
    "                while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                        text[valid_start]):\n",
    "                    valid_start += 1\n",
    "                while valid_end > 1 and invalid_span_tokens.match(\n",
    "                        text[valid_end - 1]):\n",
    "                    valid_end -= 1\n",
    "\n",
    "                valid_entities.append([valid_start, valid_end, label])\n",
    "            cleaned_data.append([text, {'entities': valid_entities}])\n",
    "        return cleaned_data\n",
    "    \n",
    "    def printEntitySpansAndLabels():\n",
    "        for i in utils.v2Tov3Converter(train_data):\n",
    "            for j in i.ents:\n",
    "                prt(j.text, \" \", j.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = utils.v2Tov3Converter(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading and tesing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'companies worked at': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)',\n",
    "}\n",
    "options = {\n",
    "    'ents': ['companies worked at'], \"colors\": colors\n",
    "}\n",
    "\n",
    "class tools:\n",
    "\n",
    "    def loadModel(model_dir = './trained_model/model-last'):\n",
    "        return spacy.load(model_dir)\n",
    "\n",
    "    def loadPdfs(dir='./data for testing'):\n",
    "        path = pl.Path(dir)\n",
    "        return list(path.glob(\"*.pdf\"))\n",
    "\n",
    "    def extractTextFromPdf(path, pdf_number):\n",
    "        pdf = fitz.open(path[pdf_number])\n",
    "        text = ''\n",
    "        for page in pdf:\n",
    "            text += str(page.getText())\n",
    "        text = \" \".join(text.split('\\n'))\n",
    "        return text\n",
    "    \n",
    "    def render(nlp, text):\n",
    "        displacy.render(nlp(text), style='ent')\n",
    "\n",
    "    def printEntitySpansAndLabels(doc):\n",
    "        for i in doc.ents:\n",
    "            print(i, \" | \", i.label_)\n",
    "    \n",
    "    def readTextFile(filepath='train.txt')->list:\n",
    "        # NOTE: ADD ONE NEWLINE AT THE END OF THE TEXT FILE\n",
    "        file = open(filepath, 'r')\n",
    "        content = file.readlines()\n",
    "        filtered = []\n",
    "        for i in range(len(content)):\n",
    "            if(content[i]!='\\n'):\n",
    "                filtered.append(content[i][:-1])\n",
    "        filter = []\n",
    "        for itm in filtered:\n",
    "            filter.append(ast.literal_eval(itm))\n",
    "        return filter\n",
    "\n",
    "    def saveToPickel(data, path='train.pkl'):\n",
    "        pickle.dump(data, open(path, 'wb'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Jitendra Babu FI/CO Consultant in Tech Mahindra - SAP FICO  \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chennai\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n",
       "</mark>\n",
       ", Tamil Nadu - Email me on Indeed: indeed.com/r/Jitendra-Babu/bc3ea69a183395ed  • Having 3.2-years of SAP experience as sap FICO Consultant • Involved in Implementation and support projects • Basic knowledge in simple finance. • Proficient in SAP's ASAP Methodology and well versed with business process, its mapping &amp; configuration in SAP • Good inter-personal skills, strong analytical ability and problem-solving capabilities • Ability to make timely and sound decisions based on logical assumptions, factual information. • Ability to work as a team member supporting co-workers and the commitment to the overall success of a group. • Work effectively with internal customers, co-workers and management. • Knowledge on integration of FI with other modules like MM and SD • Experience in GL, AP, and AR • Good communication skills with an aptitude to interact with the clients for Production support • Good Understanding of business process in Industry. • Expertise on data uploading tolls-LSMW • Good exposure on writing validation and substitution rules for business requirements and writing queries. • Interacting with the end user and finalizing the user requirement • Coordinating with the other teams for SAP integration aspects • Well exposure on designing the organization structure and setting it up in SAP in association with other members from different streams of the implementation team. • Detail oriented, quick learner, good listener with strong problem solving skills.  SAP FICO SKILL SET:  Finance • Financial Accounting- General Ledger Accounting FI-G/L (New GL), • Accounts Payable-FI-A/P, • Accounts Receivable FI-A/R  WORK EXPERIENCE  FI/CO Consultant in Tech Mahindra  SAP FICO -  2015 to Present  SAP FICO Consultant  SAP FICO -  April 2017 to May 2018  Project &amp; Role Description:  https://www.indeed.com/r/Jitendra-Babu/bc3ea69a183395ed?isid=rex-download&amp;ikw=download-top&amp;co=IN   Fossil Group, Inc., together with its subsidiaries, designs, develops, markets, and distributes consumer fashion accessories. The company's principal products include a line of men's and women's fashion watches and jewelry, handbags, small leather goods, belts, and sunglasses. It offers its products under its proprietary brands, such as FOSSIL, MICHELE, MISFIT, RELIC, SKAGEN, and ZODIAC, as well as under the licensed brands, including ARMANI EXCHANGE, CHAPS, DIESEL, DKNY, EMPORIO ARMANI, KARL LAGERFELD, KATE SPADE NEW YORK, MARC JACOBS, MICHAEL KORS, and TORY BURCH. The company sells its products through company-owned retail stores, department stores, specialty retail stores, specialty watch. Roles and Responsibilities: • Resolving Day to Day issues as well as providing Solution for better Business Processes. • Adhere to the SLA timelines • Coordinating with technical consultants for modifications in outputs and program changes • Handling various support issues be it process, configuration or functionality issue. • Migrated Transaction and Master Data using migration tool LSMW • Effective defect tracking, reporting and documenting the deliverables • Handling knowledge transfer sessions to the new comers in the team • Participation in regular team members meetings who are part of this support project and SAP FICO team in scope. • Conducting the Core-Team Training. • Configuring new payment terms • Defined new payment terms as per the business requirements for Vendors. • Actively involved in Table maintenance • Preparing the Root cause analysis, Back log report and SLA adherence report inputs to team leader from time to time. • Supporting the end users while running the Automatic Payment Program • Creating new Validations and Substitutions for posting transactions requirements. • Working closely with all members of the team to clear the backlog tickets. • Good Exposure towards Ticketing tool • Resolved Automatic payment program issues / bugs in implementing SAP OSS Notes.  DOMAIN EXPERIENCE: • Worked under Auditor for 6 months as a assistant in Tally ERP 9.0 package.  SAP FICO -  March 2015 to March 2017  Project &amp; Role Description: Ford India Private Limited manufactures, distributes, and exports cars, SUVs, sedans, and low displacement engines. It offers total maintenance, extended warranty, scheduled service, preferred insurance, and mobile service plans. The company sells its products through dealers to individuals, fleet organizations/rental companies, corporate, embassy/consulates and professionals, and government organizations; and sales and service outlets. Roles and Responsibilities: • Exposure towards value ASAP methodology • Co-ordination with core team and Preparation of Businesses Blue Print for the complete business process. • Documented in AS IS &amp; TO BE document • Involved in WRICEF elements • Involved in positive, negative &amp; random testing    • Involved in data upload • Involved in SAP customizing, configuring and processing the Business Transactions in Finance. • Configured and Customized the G/L account master records, G/L Account groups • Define field status variant. Define number range • Expertise on data uploading tolls-LSMW • Create and Maintain the Master Accounts for GL, • Creating vendors and customer master data • Configuration of automatic payment program • Exposure on writing validation and substitution rules for business requirements  Project:  EDUCATION  B.com  Degree College -  Machilipatnam, Andhra Pradesh  \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2014\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Graduation Year</span>\n",
       "</mark>\n",
       "  AG&amp;SGS Intermediate College  \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2011\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Graduation Year</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = tools.loadModel()\n",
    "pdfs = tools.loadPdfs()\n",
    "text = tools.extractTextFromPdf(pdfs, 1)\n",
    "tools.render(nlp, train_data[6][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi  |  Location\n",
      "Microsoft  |  Companies worked at\n",
      "Microsoft  |  Companies worked at\n",
      "Microsoft  |  Companies worked at\n",
      "Microsoft  |  Companies worked at\n",
      "Microsoft  |  Companies worked at\n",
      "Microsoft  |  Companies worked at\n",
      "Microsoft  |  Companies worked at\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "tools.printEntitySpansAndLabels(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
